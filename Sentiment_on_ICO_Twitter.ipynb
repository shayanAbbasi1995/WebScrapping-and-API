{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment on ICO - Twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4TjOoy+asrEJBK9juXE+C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shayanAbbasi1995/WebScrapping-and-API/blob/main/Sentiment_on_ICO_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Drive Connection"
      ],
      "metadata": {
        "id": "x2lSEJR1P1iz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ezI3HC-DFd2",
        "outputId": "9e55d56e-043e-4520-ef40-22decd68212e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Run this if running in Google Collab\n",
        "# Mount google drive if running from Google Collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set current directory if running from Google Collab\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/twitter_twint')# here use your path to current notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data from twitter/drive"
      ],
      "metadata": {
        "id": "Ow9oNFAVP6dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/woluxwolu/twint.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRDGsxGBRnJY",
        "outputId": "a6e52902-76a1-4089-c106-f53e9b8faad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/woluxwolu/twint.git\n",
            "  Cloning https://github.com/woluxwolu/twint.git to /tmp/pip-req-build-k0ms6ui4\n",
            "  Running command git clone -q https://github.com/woluxwolu/twint.git /tmp/pip-req-build-k0ms6ui4\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 18.2 MB/s \n",
            "\u001b[?25hCollecting aiodns\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (4.6.3)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 53.5 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.1.2-py3-none-any.whl (372 kB)\n",
            "\u001b[K     |████████████████████████████████| 372 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (1.3.5)\n",
            "Collecting aiohttp_socks\n",
            "  Downloading aiohttp_socks-0.7.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (from twint==2.1.21) (1.17.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Collecting googletransx\n",
            "  Downloading googletransx-2.4.2.tar.gz (13 kB)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->twint==2.1.21) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint==2.1.21) (2.21)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint==2.1.21) (4.1.1)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint==2.1.21) (2.10)\n",
            "Collecting python-socks[asyncio]<3.0.0,>=2.0.0\n",
            "  Downloading python_socks-2.0.3-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting elastic-transport<9,>=8\n",
            "  Downloading elastic_transport-8.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elastic-transport<9,>=8->elasticsearch->twint==2.1.21) (2021.10.8)\n",
            "Collecting urllib3<2,>=1.26.2\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->twint==2.1.21) (1.52)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->twint==2.1.21) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint==2.1.21) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint==2.1.21) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint==2.1.21) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->twint==2.1.21) (1.15.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: twint, fake-useragent, googletransx\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=40324 sha256=5ca13a89a53b53385e6f12184cce41a2a78e66f5998621767d6d0ab6cbb34551\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w1p6wb3i/wheels/26/ee/ae/ac1ad9d3a617abc3a7a3442b49e8fdd3dd72e1952a20726aea\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=5aca9a7dd4187b3f4a62f7bb4359eea67ba45fa643ef0c350c7aada28f27dd11\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15968 sha256=7721232d2dea3b030c8f4900c00e0fc54f019764529fce94441369d492491166\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/d5/b1/31104b338f7fd45aa8f7d22587765db06773b13df48a89735f\n",
            "Successfully built twint fake-useragent googletransx\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, python-socks, asynctest, async-timeout, aiosignal, requests, pycares, elastic-transport, aiohttp, schedule, googletransx, fake-useragent, elasticsearch, dataclasses, cchardet, aiohttp-socks, aiodns, twint\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiodns-3.0.0 aiohttp-3.8.1 aiohttp-socks-0.7.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 cchardet-2.1.7 dataclasses-0.6 elastic-transport-8.1.2 elasticsearch-8.1.2 fake-useragent-0.1.11 frozenlist-1.3.0 googletransx-2.4.2 multidict-6.0.2 pycares-4.1.2 python-socks-2.0.3 requests-2.27.1 schedule-1.1.0 twint-2.1.21 urllib3-1.26.9 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import twint\n",
        "import pandas as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "e-UYGG31Pwx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializations\n",
        "cli_voc = ['#ICO']\n",
        "years = [2021]             # Notice: to 2021\n",
        "monthes = list(range(1, 13))\n",
        "\n",
        "l = 'en'"
      ],
      "metadata": {
        "id": "FvMuvlLcle4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MonthLastDay(month):\n",
        "  # Getting the number of last day in a month - 1\n",
        "  m31 = [1,3,5,7,8,10,12]\n",
        "  m28 = [2]\n",
        "  if month in m31:\n",
        "    return 30\n",
        "  elif month in m28:\n",
        "    return 27\n",
        "  else:\n",
        "    return 29"
      ],
      "metadata": {
        "id": "6XBNKJyifFic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweets(query, since, until, lang, save_dir):\n",
        "  #configuration\n",
        "  config = twint.Config()\n",
        "  config.Search = query\n",
        "  config.Lang = lang\n",
        "  config.Since = str(since)\n",
        "  config.Until = str(until)\n",
        "  config.Show_hashtags = True\n",
        "  config.Count = True\n",
        "  config.Store_csv = True\n",
        "  config.Output = save_dir\n",
        "  #running search\n",
        "  twint.run.Search(config)"
      ],
      "metadata": {
        "id": "QwrtVfsQhFCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for y in years:\n",
        "  for m in monthes:\n",
        "    last_d = MonthLastDay(m) #Last day minus 1\n",
        "    for d in list(range(1,last_d)):\n",
        "      since = datetime.datetime(y, m, d)\n",
        "      until = datetime.datetime(y, m, d+1)\n",
        "\n",
        "      for s in cli_voc:\n",
        "        save_dir = f\"./data/twitter_scrapped_{s}_{y}_{m}_{d}_en.csv\"\n",
        "        get_tweets(s, since, until, l, save_dir)\n",
        "      \n"
      ],
      "metadata": {
        "id": "61iTWlgIPwos"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}